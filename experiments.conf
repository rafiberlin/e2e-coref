# Word embeddings.
glove_300d {
  path = glove.840B.300d.txt
  size = 300
}
glove_300d_filtered {
  path = glove.840B.300d.txt.filtered
  size = 300
}
glove_300d_2w {
  path = glove_50_300_2.txt
  size = 300
}

# Distributed training configurations.
two_local_gpus {
  addresses {
    ps = [localhost:2222]
    worker = [localhost:2223, localhost:2224]
  }
  gpus = [0, 1]
}

# Main configuration.
best {
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = train.english.jsonlines
  eval_path = dev.english.jsonlines
  conll_eval_path = dev.english.gold_conll
  lm_path = ernie_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 5000
  report_frequency = 100
  training_loop = 200000
  log_root = logs
  cluster = ${two_local_gpus}
  debug = false
  min_loss_to_break_training = 1
  max_f1 = 0
}

# For evaluation. Do not use for training (i.e. only for predict.py, evaluate.py, and demo.py). Rename `best` directory to `final`.
final = ${best} {
  context_embeddings = ${glove_300d}
  head_embeddings = ${glove_300d_2w}
  lm_path = ""
  eval_path = test.english.jsonlines
  conll_eval_path = test.english.gold_conll
}

ontonotes = ${best} {
  lm_path = "elmo_cache.hdf5"
  train_path = train.english.ontonotes.jsonlines
  eval_path = dev.english.ontonotes.jsonlines
  conll_eval_path = dev.english.v4_gold_conll
  eval_frequency = 1000
}

twiconv_wb = ${best} {
  genres = ["NOTUSED4", "NOTUSED5", "NOTUSED1", "NOTUSED2", "NOTUSED3", "NOTUSED6", "wb"]
  lm_path = "elmo_cache.hdf5"
  eval_path = dev.english.jsonlines
  conll_eval_path = dev.english.gold_conll
  eval_frequency = 500
}

twiconv_wb_eval = ${twiconv_wb} {
  comment = "Download model :  https://drive.google.com/file/d/1hXX-fE0zw3Rl0_LWazCTotkit2KhNxaE/view?usp=sharing"
  comment2 = "Trained for 80000 iterations F1 Average is 62"
  lm_path = ""
  eval_path = test.english.jsonlines
  conll_eval_path = test.english.gold_conll
}


no_wb_glove_300d_filtered {
  path = no_wb.glove.840B.300d.txt.filtered
  size = 300
}

no_wb_template = ${best} {
  genres = ["NOTUSED4", "NOTUSED5", "NOTUSED1", "NOTUSED2", "NOTUSED3", "NOTUSED6", "wb"]
  lm_path = "no_wb.elmo_cache.hdf5"
  char_vocab_path = "no_wb.char_vocab.english.txt"
  context_embeddings = ${no_wb_glove_300d_filtered}
  train_path = no_wb.train.english.jsonlines
  eval_path = no_wb.dev.english.jsonlines
  conll_eval_path = no_wb.dev.english.gold_conll
  eval_frequency = 500
}


twiconv = ${best} {
  genres = ["NOTUSED4", "NOTUSED5", "NOTUSED1", "NOTUSED2", "NOTUSED3", "NOTUSED6", "wb"]
  lm_path = "no_wb.elmo_cache.hdf5"
  char_vocab_path = "no_wb.char_vocab.english.txt"
  context_embeddings = ${no_wb_glove_300d_filtered}
  train_path = no_wb.train.english.jsonlines
  remark = "The next config is no mistake. TwiConv has no dev set. Generate the cache with python cache_elmo.py no_wb.train.english.jsonlines dev.english.ontonotes.jsonlines"
  eval_path = dev.english.ontonotes.jsonlines
  conll_eval_path = dev.english.v4_gold_conll
  eval_frequency = 500
}

twiconv_eval = ${twiconv} {
  comment = "Traine for 67500 Iterations, F1 Average is 58.4"
  comment2 = " Model stored at:https://drive.google.com/file/d/1YhURh3QnV1g2kZsPDwu0Ipmo9n8upuB9/view?usp=sharing"
  comment3 = "Need to change the directory structure after unzipping from logs/twiconv to logs/twiconv_wb"
  comment4 = "I need to use these embeddings, as I trained the model before intoducing the second dataset with "no_wb." prefix "
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  lm_path = ""
  eval_path = no_wb.test.english.jsonlines
  conll_eval_path = no_wb.test.english.gold_conll
}


twiconv_allspoken = ${no_wb_template} {
  genres = ["bc", "bn", "NOTUSED1", "NOTUSED2", "NOTUSED3", "tc", "NOTUSED4"]
}


twiconv_allspoken_eval = ${twiconv_allspoken} {
  comment = "TODO"
  comment2 = "TODO"
  lm_path = ""
  eval_path = no_wb.test.english.jsonlines
  conll_eval_path = no_wb.test.english.gold_conll
}



twiconv_bc = ${no_wb_template} {
  genres = ["bc", "NOTUSED4", "NOTUSED1", "NOTUSED2", "NOTUSED3", "NOTUSED5", "wb"]
}

twiconv_bc_eval = ${twiconv_bc} {
  comment = "TODO"
  comment2 = "TODO"
  lm_path = ""
  eval_path = no_wb.test.english.jsonlines
  conll_eval_path = no_wb.test.english.gold_conll

}


twiconv_bn = ${no_wb_template} {
  genres = ["NOTUSED4", "bn", "NOTUSED1", "NOTUSED2", "NOTUSED3", "NOTUSED5", "wb"]
}

twiconv_bn_eval = ${twiconv_bn} {
  comment = "TODO"
  comment2 = "TODO"
  lm_path = ""
  eval_path = no_wb.test.english.jsonlines
  conll_eval_path = no_wb.test.english.gold_conll
}


twiconv_tc = ${no_wb_template} {
  genres = ["NOTUSED4", "NOTUSED5", "NOTUSED1", "NOTUSED2", "NOTUSED3", "tc", "wb"]
}

twiconv_tc_eval = ${twiconv_tc} {
  comment = "Trained for 95500 iterations, Best Val F1 Average is 70.18, Test F1 Average is 66.91"
  comment2 = "https://drive.google.com/file/d/1QdOCAVIFT8wQtDfRtnblupB_EX5aqkdC/view?usp=sharing"
  lm_path = ""
  eval_path = no_wb.test.english.jsonlines
  conll_eval_path = no_wb.test.english.gold_conll
}

extract_span = ${best} {
  context_embeddings = ${glove_300d}
  head_embeddings = ${glove_300d_2w}
  lm_path = ""
  eval_path = test.english.jsonlines
  conll_eval_path = test.english.gold_conll
  debug = false
}

# Baselines.
c2f_100_ant = ${best} {
  max_top_antecedents = 100
}
c2f_250_ant = ${best} {
  max_top_antecedents = 250
}
c2f_1_layer = ${best} {
  coref_depth = 1
}
c2f_3_layer = ${best} {
  coref_depth = 3
}
distance_50_ant = ${best} {
  max_top_antecedents = 50
  coarse_to_fine = false
  coref_depth = 1
}
distance_100_ant = ${distance_50_ant} {
  max_top_antecedents = 100
}
distance_250_ant = ${distance_50_ant} {
  max_top_antecedents = 250
}
